GPT 3 - 175 Billion parameters, 96 transformer layers
GPT 3 pre training cost is 4.6 billion dollars
300 Billion tokens/words
Diverse corpus dataset
Common crawl - 410 billion tokens

- GPT 3 is few shot learner
- GPT 4 is zero shot and few shot learner

## Zero shot
Ability to generalize completely unseen tasks without prior examples

## Few shot
Learning based few examples from the user input

## Utilizing large datasets

Token is the unit of text the model reads.
Open source LAMA 3 is also giving good performance compared to closed GPT models

## GPT Architecture

Next word prediction: Self supervised learning
Auto regressive models: Previous output will be used as the input for the next prediction
Next word is used as the label

- GPT is trained only for predicting next word but it also develops other capabilities 
 like language translation on its own, this behavior is emergent behavior (Active research)




