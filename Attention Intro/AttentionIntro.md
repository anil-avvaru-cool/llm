## Attention
To process large sentences, attention is required to capture the meaning

## RNN - Recurrent neural network
- RNN used for language translation

### RNN limitation
 Doesn't handle long range dependencies 
 Loss of context: RNN doesn't have access to previous hidden state from encoder during decoding phase.

 ## Attention Mechanism
 Decoder has access to all the parts of the previous hidden state to create attention weights selectively.

 ## Self Attention
 Ability to compute attention weights by relating different positions in a single input sequence 