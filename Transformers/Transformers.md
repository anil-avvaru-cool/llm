## Transformers 
Original transformer 2017 paper: To convert English text to French and german

## Encoder
Encodes input text into vector 

## Decoder
Generates outout text from encoded vectors

## Self attention mechanism
Weigh importance of words/tokens relative to each other.
Enables model to capture long range dependencies 

## Transformer later variations
- BERT: Bi directional encoder representations from transformers
  Example: sentiment analysis
- GPT : Generative pre trained transformers

## GPT:
GPT doesn't have a encoder

## Transformers vs LLMs
Transformers can also be used for computer vision
Not all transformers are LLMs
Not all LLMs are transformers
